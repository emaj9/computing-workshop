{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating a neural network\n",
    "\n",
    "Follow the worksheet in order to simulate the use and training of a neural network.\n",
    "Ignore the following two cells. They contain the implementation of the neural network learner that you will be using in order to do the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# The sigmoid function. We use this as an activation function.\n",
    "# Used for feedforward.\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "# The derivative of the sigmoid function. Used for backprop.\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def cost(actual, expected):\n",
    "    \"\"\"Evaluates the error of a neuron.\n",
    "    This is the quadratic cost function.\n",
    "    \"\"\"\n",
    "    return 0.5 * (actual - expected) ** 2\n",
    "\n",
    "def cost_prime(actual, expected):\n",
    "    \"\"\"Derivative of the cost function wrt the actual activation.\n",
    "    The derivative of the quadratic cost function just works out to a difference.\n",
    "    \"\"\"\n",
    "    return actual - expected\n",
    "        \n",
    "def scale_gradient(k, g):\n",
    "    g2 = []\n",
    "    for layer in g:\n",
    "        l = []\n",
    "        for (b, ns) in layer:\n",
    "            l.append(\n",
    "                (k * b, [k * x for x in ns])\n",
    "            )\n",
    "        g2.append(l)\n",
    "    return g2\n",
    "            \n",
    "def add_gradient(g1, g2):\n",
    "    \"\"\"Sums two gradients.\"\"\"\n",
    "    g = []\n",
    "    for layer_l, layer_r in zip(g1, g2):\n",
    "        n = []\n",
    "        for (b_l, ns_l), (b_r, ns_r) in zip(layer_l, layer_r):\n",
    "            n.append(\n",
    "                (b_l + b_r, [x + y for x, y in zip(ns_l, ns_r)])\n",
    "            )\n",
    "        g.append(n)\n",
    "    return g\n",
    "\n",
    "class Neuron:\n",
    "    # A neuron in a NN has:\n",
    "    # - a bias\n",
    "    # - a weight for each connection to a neuron in the previous layer as well as one bias. \n",
    "    # - an index, specifying which neuron it is in its layer\n",
    "    \n",
    "    def __init__(self, j, N, weights=None, bias=None):\n",
    "        # How many neurons are we connected to in the previous layer?\n",
    "        self.N = N\n",
    "        self.index = j\n",
    "        self.debug_mode = True\n",
    "        self.prev_layer = None\n",
    "        self.next_layer = None\n",
    "\n",
    "        # Initialize the weights randomly, unless weights and bias are provided.\n",
    "        if weights is not None and bias is not None:\n",
    "            self.weights = weights\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.weights = random.randn(self.N) + 0.5 \n",
    "        self.bias = 0\n",
    "        \n",
    "    def debug(self, *args, **kwargs):\n",
    "        \"\"\" Prints a message if debug mode is on. \"\"\"\n",
    "        if(self.debug_mode):\n",
    "            print(*args, **kwargs)\n",
    "        \n",
    "    def link_back(self, layer):\n",
    "        \"\"\" Links this neuron to the neurons in the previous layer. \"\"\"\n",
    "        assert len(layer) == self.N\n",
    "        self.prev_layer = layer\n",
    "        \n",
    "    def link_forward(self, layer):\n",
    "        \"\"\" Links this neuron to the neurons in the next layers. \"\"\"\n",
    "        self.next_layer = layer\n",
    "\n",
    "    def weighted_input(self, inputs):\n",
    "        \"\"\" Computes the weighted input of this neuron. \n",
    "        This is the quantity that goes into the activation function.\n",
    "        In the literature, this is generally referred to by the letter `z`.\n",
    "        \"\"\"\n",
    "        self.z = np.dot(self.weights, inputs) + self.bias\n",
    "        return self.z\n",
    "    \n",
    "    @property\n",
    "    def inputs(self):\n",
    "        if self.prev_layer is None:\n",
    "            return np.array([])\n",
    "        return np.array([n.a for n in self.prev_layer])\n",
    "\n",
    "    def activate(self):\n",
    "        \"\"\"Computes the activation of this neuron, and stores it.\n",
    "        This in turn causes `weighted_input` to be computed and stored.\n",
    "        \"\"\"\n",
    "        if N == 0 and self.a is None:\n",
    "            raise Error(\"no activation specified in input neuron\")\n",
    "        self.a = sigmoid(self.weighted_input(self.inputs))\n",
    "        return self.a\n",
    "    \n",
    "    def delta(self, expected=None):\n",
    "        \"\"\"Computes the error of this neuron.\n",
    "        If this is an output neuron, then the delta is computed using the cost function,\n",
    "        so you must supply an `expected` value.\n",
    "        Requires that the next layer have its errors already computed.\n",
    "        \"\"\"\n",
    "        if expected is None:\n",
    "            self.d = sum(\n",
    "                n.d * n.weights[self.index] * sigmoid_prime(self.z)\n",
    "                for n\n",
    "                in self.next_layer\n",
    "            )\n",
    "        else:\n",
    "            self.d = cost_prime(self.a, expected) * sigmoid_prime(self.z)\n",
    "            \n",
    "        return self.d\n",
    "    \n",
    "class Network:\n",
    "    def __init__(self, input_layer_size, hidden_layers, output_layer):\n",
    "        # the network is just a tuple of layers. Each layer is a list of neurons.\n",
    "        self._network = \\\n",
    "            ([Neuron(j, 0) for j in range(input_layer_size)],) + \\\n",
    "            tuple(hidden_layers) + \\\n",
    "            (output_layer,) # stick output layer on the end\n",
    "        \n",
    "        #link each neuron in each layer to its preceding layer\n",
    "        #the input layer has no preceding layer, so its neurons are not linked to anything.\n",
    "        for prev_layer, layer in zip(self._network, self._network[1:]):\n",
    "            for neuron in layer:\n",
    "                neuron.link_back(prev_layer)\n",
    "                \n",
    "        for layer, next_layer in zip(self._network, self._network[1:]):\n",
    "            for neuron in layer:\n",
    "                neuron.link_forward(next_layer)\n",
    "                \n",
    "    def reset(self):\n",
    "        for layer in self._network[1:]:\n",
    "            for neuron in layer:\n",
    "                neuron.reset()\n",
    "                \n",
    "    @property\n",
    "    def input_layer(self):\n",
    "        return self._network[0]\n",
    "              \n",
    "    @property\n",
    "    def input(self):\n",
    "        return tuple(neuron.a for neuron in self.input_layer)\n",
    "    \n",
    "    @input.setter\n",
    "    def input(self, activations):\n",
    "        assert len(activations) == len(self.input_layer)\n",
    "        for neuron, a in zip(self.input_layer, activations):\n",
    "            neuron.a = a\n",
    "        \n",
    "    @property\n",
    "    def output(self):\n",
    "        return np.array([n.a for n in self._network[-1]])\n",
    "    \n",
    "    def feedforward(self, activations):\n",
    "        \"\"\" Computes a prediction for the given input activations. \"\"\"\n",
    "        self.input = activations\n",
    "        for layer in self._network[1:]:\n",
    "            for neuron in layer:\n",
    "                neuron.activate()\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, expected):\n",
    "        \"\"\" Computes the errors at each neuron for the given expected output vector. \"\"\"\n",
    "        input, *layers, out = self._network\n",
    "        for neuron, y in zip(out, expected):\n",
    "            neuron.delta(y)\n",
    "        for layer in reversed(layers):\n",
    "            for neuron in layer:\n",
    "                neuron.delta()\n",
    "                \n",
    "    @property\n",
    "    def gradient(self):\n",
    "        \"\"\"Computes the gradient of the cost function once all the errors have been calculated.\n",
    "        The gradient is represented in the same shape as the network.\n",
    "        Each neuron's portion of the gradient is represented by a tuple with the components:\n",
    "        1. the partial derivative of the cost function wrt the neuron's bias\n",
    "        2. the partial derivative of the cost function wrt each of the weights, in order.\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        for layer in self._network[1:]:\n",
    "            neurons = []\n",
    "            for neuron in layer:\n",
    "                neurons.append(\n",
    "                    (neuron.d, [n.a * neuron.d for n in neuron.prev_layer])\n",
    "                )\n",
    "            layers.append(neurons)\n",
    "        return layers\n",
    "        \n",
    "    def sgd(self, batch, rate=0.1):\n",
    "        \"\"\"Performs stochastic gradient descent on the given batch.\n",
    "        The batch is a list of pairs of input and expected output.\n",
    "        \"\"\"\n",
    "        gradients = []\n",
    "        for input, output in batch:\n",
    "            self.feedforward(input)\n",
    "            self.backprop(output)\n",
    "            gradients.append(self.gradient)\n",
    "        g = reduce(add_gradient, gradients)\n",
    "        g = scale_gradient(float(rate) / len(batch), g)\n",
    "        self.apply_gradient(g)\n",
    "        \n",
    "    def apply_gradient(self, g):\n",
    "        for layer, l in zip(self._network[1:], g):\n",
    "            for neuron, (b, ns) in zip(layer, l):\n",
    "                neuron.bias -= b\n",
    "                neuron.weights = np.array(\n",
    "                    [w - w_ for w, w_ in zip(neuron.weights, ns)]\n",
    "                )\n",
    "    \n",
    "    def evaluate(self, samples):\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        for input, output in samples:\n",
    "            self.feedforward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# The targets in the iris dataset are 0, 1, 2\n",
    "# but our neural network wants a triple of values between zero and one (activations).\n",
    "# So we make a dictionary that associates *labels* (numbers 0, 1, 2) to an output our NN can understand.\n",
    "\n",
    "nn_target_map = {\n",
    "    0: np.array([1, 0, 0]),\n",
    "    1: np.array([0, 1, 0]),\n",
    "    2: np.array([0, 0, 1]),\n",
    "}\n",
    "\n",
    "nn_targets = []\n",
    "for t in iris['target']:\n",
    "    nn_targets.append(\n",
    "        nn_target_map[t]\n",
    "    )\n",
    "# now nn_targets contains appropriate outputs\n",
    "\n",
    "# samples will be the list of input-output pairs to train the network on\n",
    "samples = list(zip(iris['data'], nn_targets))\n",
    "\n",
    "# we also need a way to convert the NN output back into the representation that we started with\n",
    "# The NN will output a vector of three numbers:\n",
    "# our conversion will take the index of the greatest value in the vector\n",
    "def convert_back(a):\n",
    "    greatest_value = float('-inf')\n",
    "    greatest_index = -1\n",
    "    for i, x in enumerate(a):\n",
    "        if x > greatest_value:\n",
    "            greatest_value = x\n",
    "            greatest_index = i\n",
    "    return greatest_index\n",
    "\n",
    "# Calculates the accuracy rating of the given network\n",
    "def evaluate(net, samples):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for input, actual in samples:\n",
    "        actual = convert_back(actual)\n",
    "        predicted = convert_back(net.feedforward(input))\n",
    "        if actual == predicted:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    return correct / len(samples)\n",
    "            \n",
    "def train(net, samples, iterations, mini_batch_size=10, learning_rate=0.1):\n",
    "    for _ in range(iterations):\n",
    "        batch = sample(samples, k=mini_batch_size)\n",
    "        net.sgd(batch, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "# Prepare the network\n",
    "\n",
    "# input layer size\n",
    "N = 4\n",
    "\n",
    "# first hidden layer\n",
    "p1 = Neuron(0, N)\n",
    "p2 = Neuron(1, N)\n",
    "layer1 = [p1, p2]\n",
    "\n",
    "# output layer\n",
    "o1 = Neuron(0, 2)\n",
    "o2 = Neuron(1, 2)\n",
    "o3 = Neuron(2, 2)\n",
    "output = [o1, o2, o3]\n",
    "\n",
    "# make the network\n",
    "net = Network(N, [layer1], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do a feedforward\n",
    "\n",
    "Move on to the next subsection only when every cell in the current subsection has been evaluated by the person responsible for it in your team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEADER:\n",
    "net.input = samples[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer activates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer activates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEADER:\n",
    "net.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this compare to the expected output?\n",
    "# Let's look at the expected output:\n",
    "expected = samples[0][1]\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do backpropagation to update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer computes errors\n",
    "\n",
    "The output layer uses the expected values at their layer to determine what their error is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1.delta(expected[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2.delta(expected[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.delta(expected[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer computes errors\n",
    "\n",
    "The hidden layers use the errors of the next layer (in this case the output layer) to determine what their error is. That's why their call to `delta` doesn't take any arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.delta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p2.delta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the gradient\n",
    "\n",
    "The gradient represents how sensitive the cost function is changes in each of the parameters (weights or biases) in the network. The gradient is computed from the errors, which is why we needed to calculate and propagate them backwards before we could get to this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LEADER:\n",
    "g = net.gradient\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the gradient by the learning rate\n",
    "\n",
    "We want to apply the gradient to the weights and biases to adjust them all so as to make the cost for this training example smaller. But we can't just apply it as is, since it might \"overshoot\" the optimal parameters.\n",
    "To rectify this, we scale the gradient by a _learning rate_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEADER:\n",
    "learning_rate = 0.1 # shrink each element of the gradient by a factor of 10.\n",
    "g = scale_gradient(learning_rate, g)\n",
    "g # confirm that things are smaller by 10x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the gradient to the network\n",
    "\n",
    "Now we can apply the gradient to the network to actually adjust the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEADER:\n",
    "net.apply_gradient(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do another feed forward (go back to the step labelled \"do a feed forward\") and see if the output of the network improved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "You may have noticed that the change in output was very slight. That's because we take very very tiny steps along the gradient each time we compute one. Again, we want to avoid taking steps that are too large, or else the optimization of the cost function risks not converging. In practice, this means that on every iteration the parameters change wildly and don't \"settle down\" on any particular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.reset()\n",
    "train(net, samples, iterations=7000) # train the network for 7000 iterations\n",
    "evaluate(net, samples) # compute the accuracy of the network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
