{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Demo of DBSCAN clustering algorithm\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds core\n",
    "samples in regions of high density and expands clusters from them. This\n",
    "algorithm is good for data which contains clusters of similar density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We use `sklearn.datasets.make_blobs` to create 3 synthetic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Choose three centers for the blobs we're going to generate\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "# Generate 750 total random points, around the centers, with a given standard deviation 0.4\n",
    "X, labels_true = make_blobs(\n",
    "    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n",
    ")\n",
    "# Normalize the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "# Uncomment the following line and press shift+enter to learn more about this normalization\n",
    "# StandardScaler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the resulting data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute DBSCAN\n",
    "\n",
    "Now we'll run a DBSCAN on the artifical points we generated in the previous part.\n",
    "\n",
    "How many clusters should we expect the algorithm to find?\n",
    "\n",
    "Fill in the blanks (`???`) of the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN # The clustering algorithm\n",
    "from sklearn import metrics\n",
    "\n",
    "# Step 1. Initialize the clustering algorithm DBSCAN and set the parameters `eps=0.3` and `min_samples=10`.\n",
    "db = ???\n",
    "\n",
    "# Perform the clustering. To do this, use the `.fit()` function of `db`\n",
    "# and pass in the dataset X constructed in part 1\n",
    "???\n",
    "\n",
    "# We can access the labels that the clustering inferred for the points.\n",
    "# This will be an array of the same size as X.\n",
    "labels = db.labels_\n",
    "# labels[0] will be the inferred label of X[0], and so on.\n",
    "\n",
    "# The presence of a label `-1` indicates that there were points that the clustering couldn't classify.\n",
    "# These are called 'noise'.\n",
    "\n",
    "# Count the number of inferred classes.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "# Count the 'noise' points that couldn't be classified.\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms are fundamentally unsupervised learning methods.\n",
    "However, since `make_blobs` gives access to the true\n",
    "labels of the synthetic clusters, it is possible to use evaluation metrics\n",
    "that leverage this \"supervised\" ground truth information to quantify the\n",
    "quality of the resulting clusters. Examples of such metrics are the\n",
    "homogeneity, completeness, V-measure, Rand-Index, Adjusted Rand-Index and\n",
    "Adjusted Mutual Information (AMI).\n",
    "\n",
    "If the ground truth labels are not known, evaluation can only be performed\n",
    "using the model results itself. In that case, the Silhouette Coefficient comes\n",
    "in handy as a metric for evaluating the result of an unsupervised learning technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Homogeneity: {metrics.homogeneity_score(labels_true, labels):.3f}\")\n",
    "print(f\"Completeness: {metrics.completeness_score(labels_true, labels):.3f}\")\n",
    "print(f\"V-measure: {metrics.v_measure_score(labels_true, labels):.3f}\")\n",
    "print(f\"Adjusted Rand Index: {metrics.adjusted_rand_score(labels_true, labels):.3f}\")\n",
    "print(\n",
    "    \"Adjusted Mutual Information:\"\n",
    "    f\" {metrics.adjusted_mutual_info_score(labels_true, labels):.3f}\"\n",
    ")\n",
    "print(f\"Silhouette Coefficient: {metrics.silhouette_score(X, labels):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results\n",
    "\n",
    "Core samples (large dots) and non-core samples (small dots) are color-coded\n",
    "according to the asigned cluster. Samples tagged as noise are represented in\n",
    "black.\n",
    "\n",
    "Formally, what is a core point? (following https://en.wikipedia.org/wiki/DBSCAN#Preliminary)\n",
    "Intuitively, what is a core point and what is a non-core point?\n",
    "\n",
    "Run the following cell to visualize which points in the generated dataset are core points, which are non-core points, and which are noise points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "\n",
    "plt.title(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
