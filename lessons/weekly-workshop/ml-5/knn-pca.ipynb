{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5t4MwlhF6qP5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-Loy8S975Kr"
   },
   "source": [
    "# 1. Examining the data\n",
    "\n",
    "We will be using k-nearest neighbours to classify and cluster a dataset of faces. We're using the Olivetti dataset, which contains 400 images of 40 people (10 images per person). Each image is 64x64 and in black and white.\n",
    "\n",
    "Is KNN a good approach to classifying the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "tQRDDnX98ZT0",
    "outputId": "009b654b-3c40-4e2d-a1fa-c67804f9c1c3"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "x,y = datasets.fetch_olivetti_faces(return_X_y=True)\n",
    "def plot_images(data):\n",
    "    num_plots = data.shape[0]\n",
    "    fig = plt.figure(figsize=(num_plots, 10.*num_plots))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(1, num_plots), axes_pad=0.1)\n",
    "    for i in range(num_plots):\n",
    "        grid[i].imshow(data[i].reshape((64,64)))\n",
    "    plt.show()\n",
    "plot_images(x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YV63Mu48jo7"
   },
   "source": [
    "# 2. Designing the model\n",
    "\n",
    "Write the steps (or pseudocode) for how to find the k-nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4msrAN_I-14j"
   },
   "source": [
    "# Testing the code\n",
    "\n",
    "Find the 10 nearest neighbours of the first face in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwI3RMN09gNj"
   },
   "source": [
    "# 3. Hyperparameter tuning\n",
    "\n",
    "Questions:\n",
    "\n",
    "*   What is the hyperparameter?\n",
    "*   How would we determine the best value for the hyperparameter?\n",
    "*   What range of values should we consider?\n",
    "*   Pseudocode\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXHGND7t7OyC"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def shuffle_and_get_val(x,y,rs,val_size):\n",
    "    x_shuffle,y_shuffle = shuffle(x, y, random_state = rs)\n",
    "    x_val,y_val = x_shuffle[:val_size],y_shuffle[:val_size]\n",
    "    x_train,y_train = x_shuffle[val_size:],y_shuffle[val_size:]\n",
    "\n",
    "    return x_train,y_train,x_val,y_val\n",
    "\n",
    "def find_best_k(r,x_train,y_train,x_test,y_test):\n",
    "    '''\n",
    "    Returns the k that yields the best accuracy for KNNS classification, and its\n",
    "    accuracy. If two k's yield the same accuracy, returns the larger one to reduce overfitting.\n",
    "    r = range of k's to check accuracy for\n",
    "    '''\n",
    "    test_size = np.shape(y_test)[0]\n",
    "    best_k = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    for j in range(1,r+1):\n",
    "        knn = KNeighborsClassifier(n_neighbors=j)\n",
    "        knn.fit(x_train, y_train)\n",
    "        y = knn.predict(x_test)\n",
    "\n",
    "        acc = np.sum(y == y_test)/test_size\n",
    "\n",
    "        if acc >= best_acc:\n",
    "            best_k = j\n",
    "            best_acc = acc\n",
    "\n",
    "    return best_k, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dimensionality-reduced data and get training and test set\n",
    "x_train,y_train,x_val,y_val = shuffle_and_get_val(x,y,2,80)\n",
    "\n",
    "# optimize for K from 1-10\n",
    "best_k, best_acc = find_best_k(10,x_train,y_train,x_val,y_val)\n",
    "\n",
    "print(f'best validation accuracy is {best_acc*100} percent, using k={best_k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIB0KQA7_tPN"
   },
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Each face image has 4096 parameters (pixels). But most of the variance in the data can be explained with far fewer components.\n",
    "\n",
    "We want a mapping between our data space (4096 dimensions) to a lower dimensional space, without losing too much information. I.e. we want to **faithfully** represent the data with fewer dimensions.\n",
    "\n",
    "For simplicity, we'll assume this mapping is linear, so we can describe it with a matrix $Q$. We assume $Q$ is orthonormal, with inverse $Q^T$.\n",
    "\n",
    "We want to minimize the **reconstruction error** given by $\\min_Q = \\sum_n||x^{(n)} - x^{(n)}QQ^T||_2^2$.\n",
    "\n",
    "Since $Q$ is orthogonal, we can think of it as a change of coordinates. We want to change the coordinate such that the first $n$ new coordinates best explain the data, for any $n$.\n",
    "\n",
    "So we want coordinates that emphasize the variance in the data. The first new coordinate should have the maximum variance, the second coordinate should have the second highest variance, etc.\n",
    "\n",
    "If $X$ is the matrix containing our data, let $\\sum = \\frac{1}{N}X^TX$ be the covariance matrix.\n",
    "\n",
    "It turns out that this $Q$ corresponds to the matrix of eigenvectors of $\\sum$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tan8LflkftNa"
   },
   "source": [
    "Visualizing the eigenfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "PEn2c5mx7QOj",
    "outputId": "4098d8fa-9eb3-4322-fc68-6975fd387e77"
   },
   "outputs": [],
   "source": [
    "x_norm = x - np.mean(x,axis=0) # normalizing the data\n",
    "\n",
    "u,s,vt = np.linalg.svd(x_norm, full_matrices=False) # the Singular Value Decomposition (SVD) is used to do the PCA\n",
    "plot_images(vt[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "jVjYPYzr7Re7",
    "outputId": "f3b448f5-11cb-49b1-96fe-15201b4d6c12"
   },
   "outputs": [],
   "source": [
    "variances = np.var(x_norm @ vt.T, axis=0)\n",
    "var_explained = np.cumsum(variances/np.sum(variances))\n",
    "L = np.where(var_explained >= 0.95)[0][0] + 1\n",
    "\n",
    "plt.plot(var_explained)\n",
    "plt.xlabel(\"no. principal components\")\n",
    "plt.ylabel(\"percentage of variance explained\")\n",
    "plt.show()\n",
    "print(f'95% of the variance is explained using {L} principal components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTktwJBoANRk"
   },
   "source": [
    "We repeat KNN, using the faces in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKM-9j9N7S1D",
    "outputId": "3e9630d6-85b8-42af-ce28-e4abeb25a1f5"
   },
   "outputs": [],
   "source": [
    "# convert training, test data to 100 principal components\n",
    "v = np.transpose(vt)[:,:100]\n",
    "x_dr = np.matmul(x_norm,v)\n",
    "\n",
    "# shuffle dimensionality-reduced data and get training and test set\n",
    "x_train_dr,y_train_dr,x_val_dr,y_val_dr = shuffle_and_get_val(x_dr,y,2,80)\n",
    "\n",
    "# optimize for K from 1-10\n",
    "best_k, best_acc = find_best_k(10,x_train_dr,y_train_dr,x_val_dr,y_val_dr)\n",
    "\n",
    "print(f'best validation accuracy is {best_acc*100} percent, using k={best_k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2BH2nDwAb6s"
   },
   "source": [
    "Finally, we divide the faces into k=40 clusters, and plot the images for 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f_JAJ1N67TTD",
    "outputId": "c81dcdcc-e54a-41a3-870b-372e1851d361"
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "cl = cluster.KMeans(n_clusters=40).fit(x)\n",
    "indices = np.arange(400)\n",
    "\n",
    "for i in range(10):\n",
    "  c = indices[cl.labels_[indices] == i]\n",
    "  plot_images(x[c])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
